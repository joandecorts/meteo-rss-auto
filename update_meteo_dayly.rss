def get_daily_summary(station_code, station_name):
    """ObtÃ© el resum diari (mÃ ximes, mÃ­nimes, acumulats) del dia actual"""
    try:
        # URL amb la data d'avui
        today = datetime.now().strftime('%Y-%m-%d')
        url = f"https://www.meteo.cat/observacions/xema/dades?codi={station_code}&dia={today}"

        write_log(f"ğŸŒ Consultant resum diari {station_name} [{station_code}]...")
        write_log(f"ğŸ“„ URL: {url}")

        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()

        soup = BeautifulSoup(response.content, 'html.parser')

        daily_data = {
            'station_name': station_name,
            'station_code': station_code,
            'data_consulta': today,
            'hora_consulta': datetime.now().strftime('%H:%M'),
            'periode': '00:00-24:00'
        }

        # ESTRATÃˆGIA PRINCIPAL: Cerca especÃ­fica per l'ID del resum diari
        write_log("ğŸ” Cercant div amb id='resum-diari'...")
        resum_div = soup.find('div', id='resum-diari')

        if not resum_div:
            write_log("âŒ No s'ha trobat l'element amb id='resum-diari'. Buscant per h2...")
            # Fallback: buscar per tÃ­tol H2
            h2_title = soup.find('h2', string='Resum diari')
            if h2_title:
                resum_div = h2_title.find_parent('div')  # Busca el div pare de l'h2
                if resum_div:
                    write_log("âœ… Trobat 'Resum diari' dins d'un h2, recuperat el div pare.")

        if resum_div:
            write_log("âœ… SecciÃ³ del resum diari localitzada.")

            # STRATEGY: Buscar i analitzar la TAULA de dades
            # Primer busca una taula dins del resum
            data_table = resum_div.find('table')

            if data_table:
                write_log("ğŸ“‹ Taula de dades trobada. Analitzant files...")
                rows = data_table.find_all('tr')
                for row in rows:
                    cells = row.find_all(['td', 'th'])
                    # Cada fila pot tenir, per exemple, "Temperatura mitjana" i "11.6 Â°C"
                    if len(cells) >= 2:
                        label = cells[0].get_text(strip=True).lower()
                        value_cell = cells[1].get_text(strip=True)
                        
                        # Netejar el valor (treure 'Â°C', 'mm', etc.)
                        value_num = convertir_a_numero(value_cell.split()[0] if value_cell else None)

                        if 'temperatura mitjana' in label:
                            daily_data['tm'] = value_num
                        elif 'temperatura mÃ xima' in label or 'temperatura mÃ¡xima' in label:
                            daily_data['tx'] = value_num
                        elif 'temperatura mÃ­nima' in label:
                            daily_data['tn'] = value_num
                        elif 'humitat relativa mitjana' in label:
                            daily_data['hr'] = value_num
                        elif 'precipitaciÃ³ acumulada' in label or 'precipitaciÃ³' in label:
                            daily_data['ppt'] = value_num
                            
            else:
                write_log("ğŸ” No s'ha trobat taula. Cercant per etiquetes de llista...")
                # Si no hi ha taula, les dades poden estar en <p>, <span>, o <div>
                full_resum_text = resum_div.get_text()
                write_log(f"ğŸ“„ Text complet del resum: {full_resum_text}")

                # Patrons regex per extreure dades del text pla
                patterns = {
                    'tm': r'temperatura mitjana[^\d]*([\d,\.]+)',
                    'tx': r'temperatura (?:mÃ xima|mÃ¡xima)[^\d]*([\d,\.]+)',
                    'tn': r'temperatura mÃ­nima[^\d]*([\d,\.]+)',
                    'hr': r'humitat relativa mitjana[^\d]*([\d,\.]+)',
                    'ppt': r'precipitaciÃ³ (?:acumulada)?[^\d]*([\d,\.]+)'
                }

                for key, pattern in patterns.items():
                    match = re.search(pattern, full_resum_text, re.IGNORECASE)
                    if match:
                        daily_data[key] = convertir_a_numero(match.group(1).replace(',', '.'))
                        write_log(f"   âœ… {key.upper()} (via regex): {daily_data[key]}")

        else:
            write_log("âŒ No s'ha trobat cap contenidor del resum diari.")

        # COMPROVACIÃ“ FINAL: Tenim alguna dada vÃ lida?
        dades_valides = any(daily_data.get(key) is not None for key in ['tm', 'tx', 'tn', 'hr', 'ppt'])
        if dades_valides:
            write_log("âœ… Dades del resum diari extretes amb Ã¨xit.")
            for key, value in daily_data.items():
                if value is not None and key not in ['station_name', 'station_code', 'data_consulta', 'hora_consulta', 'periode']:
                    write_log(f"   ğŸ“ {key.upper()}: {value}")
            return daily_data
        else:
            write_log("âŒ No s'han pogut extreure dades del resum diari.")
            # Per depuraciÃ³, opcionalment guarda un fragment de l'HTML
            with open(f'debug_{station_code}_resum.html', 'w', encoding='utf-8') as f:
                f.write(str(soup.prettify())[:10000])
            return None

    except Exception as e:
        write_log(f"âŒ Error consultant resum diari: {e}")
        import traceback
        write_log(f"ğŸ“‹ Traceback: {traceback.format_exc()}")
        return None
